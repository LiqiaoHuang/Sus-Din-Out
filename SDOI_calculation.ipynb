{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd83b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_nutrition = \"Nutrient_Density_Scores_Normalized_SubIndices.xlsx\"\n",
    "df_nutrition = pd.read_excel(file_path_nutrition)\n",
    "\n",
    "category_avg_nds = df_nutrition.groupby('Category')['Normalized NDS'].mean().reset_index()\n",
    "\n",
    "categorized_restaurants_path = 'Categorized_restaurants.xlsx'\n",
    "nutrition_path = '外食計算.xlsx'\n",
    "file_path_place = 'tokyo_unique_found_places.xlsx'\n",
    "\n",
    "df_place = pd.read_excel(file_path_place)\n",
    "df_restaurants = pd.read_excel(categorized_restaurants_path)\n",
    "df_carbon = pd.read_excel(nutrition_path, sheet_name='carbon')\n",
    "\n",
    "df_restaurants['FIES_category'] = df_restaurants['FIES_category'].astype(str)\n",
    "\n",
    "df_restaurants = pd.merge(\n",
    "    df_restaurants,\n",
    "    df_nutrition,   \n",
    "    left_on='FIES_category',\n",
    "    right_on='Category',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_restaurants = pd.merge(\n",
    "    df_restaurants,\n",
    "    df_place[['id', 'priceLevel', 'rating', 'userRatingCount', 'currentOpeningHours']],\n",
    "    on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_merged = df_restaurants.merge(category_avg_nds, left_on='FIES_category', right_on='Category', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Restaurant_500m_district.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df,\n",
    "    df_restaurants[['id', 'Normalized NDS', 'rating','priceLevel']], \n",
    "    how='left',        \n",
    "    on='id'            \n",
    ")\n",
    "\n",
    "columns_to_drop = [\n",
    "    'Energy_kca','Protein_g_','Lipid_g__x','Carbohydra','Salt_equiv',\n",
    "    'Protein_g1','Lipid_g__y','Energy_k_1','Carbohyd_1','Salt_equ_1','Protein__1',\n",
    "    'Lipid_g__1','Carbohyd_2','Salt_equ_2','NDS_Protei','NDS_Lipid',\n",
    "    'NDS_Carboh','NDS_Salt_e','NDS','NDS_normal'\n",
    "]\n",
    "\n",
    "df_merged.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_merged.drop_duplicates(subset=['N02_005', 'id'], keep='first')\n",
    "df_filtered = df_filtered[df_filtered['rating'] > 0]\n",
    "\n",
    "price_level_scores = {\n",
    "    'PRICE_LEVEL_INEXPENSIVE': 0.7,\n",
    "    'PRICE_LEVEL_MODERATE': 0.5,\n",
    "    'PRICE_LEVEL_EXPENSIVE': 0.3,\n",
    "    'PRICE_LEVEL_VERY_EXPENSIVE': 0.1\n",
    "}\n",
    "df_filtered['priceLevelScore'] = df_filtered['priceLevel'].map(price_level_scores)\n",
    "\n",
    "average_price_scores = df_filtered.groupby('N02_005')['priceLevelScore'].mean()\n",
    "average_rating_scores = df_filtered.groupby('N02_005')['rating'].mean()\n",
    "df=df_filtered\n",
    "\n",
    "restaurant_counts = df.groupby(['N02_005', 'primaryT_1']).size().unstack(fill_value=0)\n",
    "restaurant_totals = restaurant_counts.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = restaurant_counts.div(restaurant_totals, axis=0)\n",
    "shannon_indices = - (proportions * np.log(proportions)).sum(axis=1)\n",
    "\n",
    "shannon_indices_normalized = shannon_indices / shannon_indices.max()\n",
    "restaurant_totals_normalized = restaurant_totals / restaurant_totals.max()\n",
    "\n",
    "composite_diversity_indices = restaurant_totals_normalized * shannon_indices_normalized\n",
    "composite_diversity_indices2 = shannon_indices * restaurant_totals/restaurant_totals.max()/shannon_indices.max()\n",
    "\n",
    "average_nutrition_scores = df.groupby('N02_005')['Normalized NDS'].mean()\n",
    "\n",
    "composite_diversity_indices_root5 = np.power(composite_diversity_indices, 1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carbon_melted = df_carbon.melt(id_vars=['Type'], var_name='priceLevel', value_name='Carbon')\n",
    "\n",
    "df_carbon_melted = df_carbon_melted.dropna(subset=['Carbon'])\n",
    "\n",
    "merged_with_carbon = pd.merge(df, df_carbon_melted, left_on=['FIES_categ', 'priceLevel'], right_on=['Type', 'priceLevel'], how='left')\n",
    "\n",
    "merged_with_carbon['Carbon'] = merged_with_carbon['Carbon'].fillna(0)\n",
    "\n",
    "max_carbon = merged_with_carbon['Carbon'].max()\n",
    "min_carbon = merged_with_carbon['Carbon'].min()\n",
    "\n",
    "merged_with_carbon['Carbon_Normalized'] = (max_carbon - merged_with_carbon['Carbon']) / (max_carbon - min_carbon)\n",
    "\n",
    "average_carbon_scores = merged_with_carbon.groupby('N02_005')['Carbon'].mean()\n",
    "average_normalized_carbon_scores = merged_with_carbon.groupby('N02_005')['Carbon_Normalized'].mean()\n",
    "\n",
    "average_normalized_carbon_scores4 = np.power(average_normalized_carbon_scores, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ed913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = pd.DataFrame({\n",
    "    'Station': restaurant_totals.index,\n",
    "    'Shannon_Index': shannon_indices,\n",
    "    'Shannon_Index_Normalized': shannon_indices_normalized,\n",
    "    'Restaurant_Total': restaurant_totals,\n",
    "    'Restaurant_Total_Normalized': restaurant_totals_normalized,\n",
    "    'Composite_Diversity_Index': composite_diversity_indices,\n",
    "    'Average_Nutrition_Score': average_nutrition_scores,\n",
    "    'Average_Price_Score': average_price_scores,\n",
    "    'Average_Rating_Score': average_rating_scores,\n",
    "    'Average_Carbon_Score': average_carbon_scores,\n",
    "    'Average_Carbon_Score_Normalized': average_normalized_carbon_scores,\n",
    "})\n",
    "\n",
    "df_stations['Composite_Diversity_Index_Root5'] = composite_diversity_indices_root5.values\n",
    "\n",
    "df_stations = df_stations.join(average_rating_scores, on='Station', rsuffix='_rating')\n",
    "\n",
    "df_stations.rename(columns={'priceLevelScore': 'Average_Price_Score'}, inplace=True)\n",
    "\n",
    "df_stations['Average_Carbon_Score_Normalized'] = df_stations['Station'].map(average_normalized_carbon_scores)\n",
    "\n",
    "df_stations['Average_Carbon_Score_Normalized'].fillna(0, inplace=True)\n",
    "\n",
    "station_info = df[['N02_005', 'NAME', 'latitude', 'longitude']].drop_duplicates('N02_005')\n",
    "\n",
    "df_stations = pd.merge(df_stations, station_info, how='left', left_on='Station', right_on='N02_005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '500buffer_Table_population.xls'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "aggregated_data = df.groupby('S1222_NumberOfPassengers_Cli2.S12_001')['S1222_NumberOfPassengers_Cli2.S12_049'].sum().reset_index()\n",
    "\n",
    "output_file_path = 'Population_processed.xlsx'\n",
    "aggregated_data.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221006a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = df_stations.merge(aggregated_data, left_on='N02_005', right_on='S1222_NumberOfPassengers_Cli2.S12_001', how='left')\n",
    "merged_data = merged_data[merged_data['S1222_NumberOfPassengers_Cli2.S12_049'] != 0]\n",
    "\n",
    "merged_data['Crowdness'] = merged_data['Restaurant_Total'] / merged_data['S1222_NumberOfPassengers_Cli2.S12_049']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c874bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0cf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from math import sin, cos, pi\n",
    "\n",
    "def calculate_area(values):\n",
    "    # Convert the values into coordinates based on angles in a pentagon\n",
    "    angles = np.linspace(0, 2 * np.pi, 5, endpoint=False)\n",
    "    coordinates = [(v * cos(a), v * sin(a)) for v, a in zip(values, angles)]\n",
    "\n",
    "    # Calculate area using the shoelace formula\n",
    "    def shoelace(coords):\n",
    "        x, y = zip(*coords)\n",
    "        return 0.5 * abs(sum(x[i] * y[i + 1] - x[i + 1] * y[i] for i in range(-1, len(coords) - 1)))\n",
    "\n",
    "    return shoelace(coordinates)\n",
    "\n",
    "# Extract the required columns from merged_data\n",
    "required_columns = ['Average_Nutrition_Score', 'Average_Price_Score', 'Average_Carbon_Score_Normalized4', 'Composite_Diversity_Index_Root5', 'Crowdness_Scaled']\n",
    "data_for_area_calculation = merged_data[required_columns]\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "# Calculate the area for each permutation for each row\n",
    "for index, row in data_for_area_calculation.iterrows():\n",
    "    row_permutations = list(set(permutations(row)))\n",
    "    areas = [calculate_area(perm) for perm in row_permutations]\n",
    "    average_area = np.mean(areas)\n",
    "    merged_data.at[index, 'Average_Area'] = average_area\n",
    "    for perm, area in zip(row_permutations, areas):\n",
    "        results.append({'Index': index, 'Permutation': perm, 'Area': area})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, merged_data, left_on='S1222_NumberOfPassengers_Cli2.S12_001', right_on='Station', how='left')\n",
    "\n",
    "merged_df.to_excel('combined_data.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
